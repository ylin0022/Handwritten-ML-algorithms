{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784)\n",
      "(30000,)\n"
     ]
    }
   ],
   "source": [
    "myFile = h5py.File('../Input/images_training.h5', 'r')\n",
    "# The '...' means retrieve the whole tensor\n",
    "X_train = myFile['data'][...]\n",
    "myLabel = h5py.File('../Input/labels_training.h5', 'r')\n",
    "y_train = myLabel['label'][...]\n",
    "X_train = X_train.reshape(len(X_train), -1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 784)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "Test = h5py.File('../Input/images_testing.h5', 'r')\n",
    "X_test = Test['data'][...]\n",
    "testLabel = h5py.File('../Input/labels_testing_2000.h5', 'r')\n",
    "y_test = testLabel['label'][...]\n",
    "X_test = X_test[:2000,:]\n",
    "X_test = X_test.reshape(len(X_test), -1)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train torch.Size([30000, 784])\n",
      "shape of y_train torch.Size([30000])\n",
      "shape of X_test torch.Size([2000, 784])\n",
      "shape of y_test torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "torch_X_train = torch.from_numpy(X_train)\n",
    "torch_y_train = torch.from_numpy(y_train)\n",
    "torch_X_test = torch.from_numpy(X_test)\n",
    "torch_y_test = torch.from_numpy(y_test)\n",
    "print('shape of X_train', torch_X_train.shape)\n",
    "print('shape of y_train', torch_y_train.shape)\n",
    "print('shape of X_test', torch_X_test.shape)\n",
    "print('shape of y_test', torch_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of trainloader: 3750\n",
      "length of testloader: 250\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(X_train, batch_size=8,\\\n",
    "                                         shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(X_test, batch_size=8,\\\n",
    "                                         shuffle=False, num_workers=2)\n",
    "classes = ('T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot')\n",
    "print('length of trainloader:', len(trainloader))\n",
    "print('length of testloader:', len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./Fashion MNIST', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./Fashion MNIST', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of trainloader: 3750\n",
      "length of testloader: 625\n"
     ]
    }
   ],
   "source": [
    "print('length of trainloader:', len(trainloader))\n",
    "print('length of testloader:', len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.BN1 = nn.BatchNorm2d(32, eps=1e-5, momentum=0.1, affine=True)\n",
    "        self.BN2 = nn.BatchNorm2d(64, eps=1e-5, momentum=0.1, affine=True)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.BN1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.BN2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = self.fc3(x)\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   750] loss: 1.071\n",
      "[1,  1500] loss: 0.508\n",
      "[1,  2250] loss: 0.448\n",
      "[1,  3000] loss: 0.396\n",
      "[1,  3750] loss: 0.368\n",
      "[2,   750] loss: 0.344\n",
      "[2,  1500] loss: 0.343\n",
      "[2,  2250] loss: 0.331\n",
      "[2,  3000] loss: 0.317\n",
      "[2,  3750] loss: 0.312\n",
      "[3,   750] loss: 0.289\n",
      "[3,  1500] loss: 0.292\n",
      "[3,  2250] loss: 0.294\n",
      "[3,  3000] loss: 0.276\n",
      "[3,  3750] loss: 0.279\n",
      "[4,   750] loss: 0.260\n",
      "[4,  1500] loss: 0.257\n",
      "[4,  2250] loss: 0.243\n",
      "[4,  3000] loss: 0.253\n",
      "[4,  3750] loss: 0.258\n",
      "[5,   750] loss: 0.234\n",
      "[5,  1500] loss: 0.230\n",
      "[5,  2250] loss: 0.233\n",
      "[5,  3000] loss: 0.237\n",
      "[5,  3750] loss: 0.235\n",
      "[6,   750] loss: 0.203\n",
      "[6,  1500] loss: 0.206\n",
      "[6,  2250] loss: 0.221\n",
      "[6,  3000] loss: 0.221\n",
      "[6,  3750] loss: 0.226\n",
      "[7,   750] loss: 0.201\n",
      "[7,  1500] loss: 0.197\n",
      "[7,  2250] loss: 0.194\n",
      "[7,  3000] loss: 0.206\n",
      "[7,  3750] loss: 0.208\n",
      "[8,   750] loss: 0.174\n",
      "[8,  1500] loss: 0.182\n",
      "[8,  2250] loss: 0.182\n",
      "[8,  3000] loss: 0.194\n",
      "[8,  3750] loss: 0.194\n",
      "[9,   750] loss: 0.164\n",
      "[9,  1500] loss: 0.168\n",
      "[9,  2250] loss: 0.173\n",
      "[9,  3000] loss: 0.180\n",
      "[9,  3750] loss: 0.178\n",
      "[10,   750] loss: 0.150\n",
      "[10,  1500] loss: 0.173\n",
      "[10,  2250] loss: 0.154\n",
      "[10,  3000] loss: 0.166\n",
      "[10,  3750] loss: 0.160\n",
      "[11,   750] loss: 0.150\n",
      "[11,  1500] loss: 0.145\n",
      "[11,  2250] loss: 0.146\n",
      "[11,  3000] loss: 0.156\n",
      "[11,  3750] loss: 0.155\n",
      "[12,   750] loss: 0.132\n",
      "[12,  1500] loss: 0.136\n",
      "[12,  2250] loss: 0.144\n",
      "[12,  3000] loss: 0.144\n",
      "[12,  3750] loss: 0.140\n",
      "[13,   750] loss: 0.118\n",
      "[13,  1500] loss: 0.131\n",
      "[13,  2250] loss: 0.135\n",
      "[13,  3000] loss: 0.131\n",
      "[13,  3750] loss: 0.138\n",
      "[14,   750] loss: 0.109\n",
      "[14,  1500] loss: 0.116\n",
      "[14,  2250] loss: 0.129\n",
      "[14,  3000] loss: 0.127\n",
      "[14,  3750] loss: 0.129\n",
      "[15,   750] loss: 0.099\n",
      "[15,  1500] loss: 0.114\n",
      "[15,  2250] loss: 0.116\n",
      "[15,  3000] loss: 0.121\n",
      "[15,  3750] loss: 0.118\n",
      "[16,   750] loss: 0.104\n",
      "[16,  1500] loss: 0.096\n",
      "[16,  2250] loss: 0.106\n",
      "[16,  3000] loss: 0.106\n",
      "[16,  3750] loss: 0.114\n",
      "Finished Training\n",
      "Wall time: 26min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "average_loss_series = []\n",
    "\n",
    "for epoch in range(16):  # loop over the dataset multiple times\n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 750 == 749:    # print every 2000 mini-batches\n",
    "            average_loss = running_loss/750\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, average_loss))\n",
    "            average_loss_series.append(average_loss)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 90.4%\n",
      "Wall time: 6.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_correct = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if i % 250 == 249:\n",
    "            test_correct.append(correct / total)\n",
    "print('Accuracy of the network on the 10000 test images: {:.1f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another better CNN with BN and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=1, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=1, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        #self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.BN1 = nn.BatchNorm2d(32, eps=1e-5, momentum=0.1, affine=True)\n",
    "        self.BN2 = nn.BatchNorm2d(64, eps=1e-5, momentum=0.1, affine=True)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 64)#pool3 要否\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.BN1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.BN1(self.conv2(x))))\n",
    "        x = self.pool2(F.relu(self.BN2(self.conv3(x))))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = self.fc3(x)\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   750] loss: 0.690\n",
      "[1,  1500] loss: 0.444\n",
      "[1,  2250] loss: 0.397\n",
      "[1,  3000] loss: 0.365\n",
      "[1,  3750] loss: 0.348\n",
      "[2,   750] loss: 0.313\n",
      "[2,  1500] loss: 0.305\n",
      "[2,  2250] loss: 0.297\n",
      "[2,  3000] loss: 0.301\n",
      "[2,  3750] loss: 0.289\n",
      "[3,   750] loss: 0.260\n",
      "[3,  1500] loss: 0.269\n",
      "[3,  2250] loss: 0.255\n",
      "[3,  3000] loss: 0.260\n",
      "[3,  3750] loss: 0.254\n",
      "[4,   750] loss: 0.219\n",
      "[4,  1500] loss: 0.240\n",
      "[4,  2250] loss: 0.236\n",
      "[4,  3000] loss: 0.234\n",
      "[4,  3750] loss: 0.225\n",
      "[5,   750] loss: 0.202\n",
      "[5,  1500] loss: 0.213\n",
      "[5,  2250] loss: 0.213\n",
      "[5,  3000] loss: 0.210\n",
      "[5,  3750] loss: 0.217\n",
      "[6,   750] loss: 0.184\n",
      "[6,  1500] loss: 0.189\n",
      "[6,  2250] loss: 0.200\n",
      "[6,  3000] loss: 0.200\n",
      "[6,  3750] loss: 0.186\n",
      "[7,   750] loss: 0.169\n",
      "[7,  1500] loss: 0.174\n",
      "[7,  2250] loss: 0.178\n",
      "[7,  3000] loss: 0.177\n",
      "[7,  3750] loss: 0.178\n",
      "[8,   750] loss: 0.154\n",
      "[8,  1500] loss: 0.149\n",
      "[8,  2250] loss: 0.167\n",
      "[8,  3000] loss: 0.167\n",
      "[8,  3750] loss: 0.176\n",
      "[9,   750] loss: 0.141\n",
      "[9,  1500] loss: 0.147\n",
      "[9,  2250] loss: 0.142\n",
      "[9,  3000] loss: 0.153\n",
      "[9,  3750] loss: 0.159\n",
      "[10,   750] loss: 0.122\n",
      "[10,  1500] loss: 0.133\n",
      "[10,  2250] loss: 0.152\n",
      "[10,  3000] loss: 0.136\n",
      "[10,  3750] loss: 0.145\n",
      "[11,   750] loss: 0.110\n",
      "[11,  1500] loss: 0.121\n",
      "[11,  2250] loss: 0.127\n",
      "[11,  3000] loss: 0.133\n",
      "[11,  3750] loss: 0.133\n",
      "[12,   750] loss: 0.102\n",
      "[12,  1500] loss: 0.108\n",
      "[12,  2250] loss: 0.118\n",
      "[12,  3000] loss: 0.132\n",
      "[12,  3750] loss: 0.132\n",
      "[13,   750] loss: 0.106\n",
      "[13,  1500] loss: 0.100\n",
      "[13,  2250] loss: 0.110\n",
      "[13,  3000] loss: 0.118\n",
      "[13,  3750] loss: 0.115\n",
      "[14,   750] loss: 0.092\n",
      "[14,  1500] loss: 0.103\n",
      "[14,  2250] loss: 0.098\n",
      "[14,  3000] loss: 0.108\n",
      "[14,  3750] loss: 0.112\n",
      "[15,   750] loss: 0.082\n",
      "[15,  1500] loss: 0.089\n",
      "[15,  2250] loss: 0.101\n",
      "[15,  3000] loss: 0.104\n",
      "[15,  3750] loss: 0.102\n",
      "[16,   750] loss: 0.074\n",
      "[16,  1500] loss: 0.080\n",
      "[16,  2250] loss: 0.089\n",
      "[16,  3000] loss: 0.091\n",
      "[16,  3750] loss: 0.097\n",
      "Finished Training\n",
      "Wall time: 1h 11min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "average_loss_series = []\n",
    "\n",
    "for epoch in range(16):  # loop over the dataset multiple times\n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 750 == 749:    # print every 1125 mini-batches\n",
    "            average_loss = running_loss/750\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, average_loss))\n",
    "            average_loss_series.append(average_loss)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 90.0%\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_correct = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if i % 250 == 249:\n",
    "            test_correct.append(correct / total)\n",
    "print('Accuracy of the network on the 10000 test images: {:.1f}%'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
